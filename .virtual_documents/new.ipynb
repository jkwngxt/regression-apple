


# Importing necessary libraries
import pandas as pd  # Import pandas for data manipulation and analysis
import numpy as np  # Import NumPy for numerical operations
from sklearn.model_selection import train_test_split  # Import train_test_split function
from sklearn.model_selection import GridSearchCV  # Importing the GridSearchCV module for hyperparameter tuning
import matplotlib.pyplot as plt  # Import matplotlib for data visualization
from sklearn.multioutput import RegressorChain  # Importing RegressorChain for multi-output regression
from sklearn import metrics  # Importing metrics for evaluating model performance

# Importing regressors
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C
from sklearn.linear_model import Lasso
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import ElasticNet
from sklearn.svm import SVR








# Import Apple.csv (dataset)
apple = pd.read_csv('Apple.csv')
apple


# Count the occurrences of each unique value in the 'region' column
apple['region'].value_counts()


# Filter rows where the region is 'Vietnam' and select only necessary columns
apple_vietnam = apple.loc[apple.region == 'Vietnam', 'Date' : 'type'].reset_index(drop=True)
apple_vietnam


# Display information about the DataFrame apple_vietnam
apple_vietnam.info()


# Check null values
apple_vietnam.isna().sum()





# Convert the data in the 'Date' column to datetime format
apple_vietnam['Date'] = pd.to_datetime(apple_vietnam['Date'], format = '%d/%m/%Y')

# Set the 'Date' column as the index and sort by index
apple_vietnam = apple_vietnam.set_index('Date').sort_index()

apple_vietnam





# Conventional Envi
con_en = apple_vietnam.loc[apple_vietnam['type'] == 'conventional' , 'Envi'].to_frame()
con_en


# Conventional Fuji
con_fu = apple_vietnam.loc[apple_vietnam['type'] == 'conventional' , 'Fuji'].to_frame()
con_fu


# Conventional gala
con_ga = apple_vietnam.loc[apple_vietnam['type'] == 'conventional' , 'Gala'].to_frame()
con_ga


# Organic Envi
org_en = apple_vietnam.loc[apple_vietnam['type'] == 'organic' , 'Envi'].to_frame()
org_en


# Organic Fuji
org_fu = apple_vietnam.loc[apple_vietnam['type'] == 'organic' , 'Fuji'].to_frame()
org_fu


# Organic Gala
org_ga = apple_vietnam.loc[apple_vietnam['type'] == 'organic' , 'Gala'].to_frame()
org_ga








# Function for plotting the first column of the DataFrame against time
def visualize_data(df, app_type):
    # Extract the name of the first column (variety of apple)
    variety = df.columns[0]
    
    # Create a figure and axis for plotting
    fig, ax = plt.subplots()
    
    # Plot the data against time
    ax.plot(df[variety])
    
    # Set labels for the x and y axes
    ax.set_xlabel('Time')
    ax.set_ylabel('Amount')
    
    # Set title for the plot
    ax.set_title(f'Visualize data : {variety} ({app_type})')

    # Automatically format the x-axis date labels for better readability
    fig.autofmt_xdate()

    # Adjust layout to prevent clipping of labels
    plt.tight_layout()





# Function for preparing dataset by adding input_length x columns and output_length y columns
def window_input_output(input_length: int, output_length: int, data:pd.DataFrame) -> pd.DataFrame:
    df = data.copy()
    i = 1
    variety = data.columns[0]
    while i < input_length:
        df[f'x_{i}'] = df[variety].shift(-i)
        i = i + 1
    j= 0
    while j < output_length:
        df[f'y_{j}'] = df[variety].shift(-output_length-j)
        j = j + 1
    df = df.dropna(axis=0)
    return df.copy()





# Function for splitting the data into training and testing sets
def define_test_train(df, test_size):
    # Assuming the first column represents the variety of data
    variety = df.columns[0]
    
    # Extracting columns starting with 'x' as features and 'y' as target variables
    X_cols = [col for col in df.columns if col.startswith('x')]
    X_cols.insert(0, variety)  # Inserting variety column at the beginning
    
    y_cols = [col for col in df.columns if col.startswith('y')]
    
    # Splitting the data into training and testing sets 
    X_train = df[X_cols][:-test_size].values
    y_train = df[y_cols][:-test_size].values
    
    X_test = df[X_cols][-test_size:].values
    y_test = df[y_cols][-test_size:].values
    
    return X_train, X_test, y_train, y_test








# Function for creating model
def create_model_SVR(X_train, y_train):
    # Define the grid of hyperparameters for SVR
    param_grid = {
        'base_estimator__kernel': ['poly','rbf', 'sigmoid'],
        'base_estimator__C': [10, 100, 1000, 10000, 100000],
        'base_estimator__epsilon': [0,1,2, 3, 4, 5, 6, 7],
        'base_estimator__gamma': ['scale', 'auto'],
        'base_estimator__coef0': [0.001,0.01,0.1,0.0,1.0,10,100,1000 ]
    }

    # Initialize GridSearchCV with RegressorChain(SVR()) and the defined parameter grid
    grid_svr = GridSearchCV(RegressorChain(SVR()), param_grid, cv=5, scoring='neg_mean_squared_error')

    # Fit GridSearchCV to find the best combination of hyperparameters
    grid_svr.fit(X_train, y_train)

    # Print the best parameters and best score obtained from GridSearchCV
    print('Best Parameters:', grid_svr.best_params_)
    print('Best Score:', grid_svr.best_score_)

    # Get the best estimator from GridSearchCV and fit it to the training data
    best = grid_svr.best_estimator_
    
    return best





# Calculate mean absolute percentage error
def mape(y_test, y_pred) :
    return round(np.mean(np.abs((y_test - y_pred) / y_test)) * 100, 2)


# Function for visualizing the predictions
def visualize_predictions(X_test,y_test,y_preds, input_length, output_length, algo) :
    print(input_length, output_length)
    print(len(y_preds))
    print(len(X_test))
    fig, ax = plt.subplots(figsize=(16, 11))
    ax.plot(np.arange(0, input_length, 1), X_test[1].reshape(-1), 'b-', label='input')
    ax.plot(np.arange(input_length, input_length + output_length, 1), y_test[1], marker='.', color='blue', label='Actual')
    ax.plot(np.arange(input_length, input_length + output_length, 1), X_test[1], marker='o', color='red', label='Baseline')
    ax.plot(np.arange(input_length, input_length + output_length, 1), y_preds[1], marker='p', color='black', label= algo)
    ax.set_xlabel('Timesteps')
    ax.set_ylabel('Amount of Apple')
    plt.legend(loc=2)
    fig.autofmt_xdate()
    plt.tight_layout()


# Function for printing evaluation metrics
def print_evaluate(true, predicted):
    # Calculate Sum of Squared Errors (SSE)
    sse = np.sum((true - predicted)**2)
    
    # Calculate Mean Absolute Error (MAE)
    mae = metrics.mean_absolute_error(true, predicted)
    
    # Calculate Mean Squared Error (MSE)
    mse = metrics.mean_squared_error(true, predicted)
    
    # Calculate Root Mean Squared Error (RMSE)
    rmse = np.sqrt(mse)
    
    # Calculate R-squared (R2) score
    r2_square = metrics.r2_score(true, predicted)
    
    # Print evaluation metrics
    print('SSE:', sse)
    print('MAE:', mae)
    print('MSE:', mse)
    print('RMSE:', rmse)
    print('R2 Square:', r2_square)
    print('__________________________________')








visualize_data(con_en, 'Conventional')





input_length = 100
output_length = 10
test_size = 1


ready_con_en = window_input_output(input_length, output_length, con_en)
ready_con_en





X_train_con_en, X_test_con_en, y_train_con_en, y_test_con_en = define_test_train(ready_con_en, test_size)
X_train_con_en, X_test_con_en, y_train_con_en, y_test_con_en





# create svr model
svr_con_en = create_model_SVR(X_train_con_en, y_train_con_en)


y_pred_con_en = svr_con_en.predict(X_test_con_en)
y_pred_con_en


print_evaluate(y_test_con_en, y_pred_con_en)


# Function for visualizing the predictions
def visualize_predictions2(X_test, y_test, y_preds, input_length, output_length, algo):
    fig, ax = plt.subplots(figsize=(16, 11))
    
    # Plot input sequence
    ax.plot(np.arange(0, input_length, 1), X_test[0].reshape(-1), 'b-', label='Input')
    
    # Plot actual output
    ax.plot(np.arange(input_length, input_length + output_length, 1), y_test[0], marker='.', color='blue', label='Actual')
    
    # Plot baseline (if applicable)
    ax.plot(np.arange(input_length, input_length + output_length, 1), X_test[0][-output_length:], marker='o', color='red', label='Baseline')
    
    # Plot predicted output
    ax.plot(np.arange(input_length, input_length + output_length, 1), y_preds[0], marker='p', color='black', label=algo)
    
    # Set labels and legend
    ax.set_xlabel('Timesteps')
    ax.set_ylabel('Amount of Apple')
    plt.legend(loc=2)
    
    # Rotate x-axis labels for better readability
    fig.autofmt_xdate()
    
    # Ensure tight layout
    plt.tight_layout()


#visualize_predictions2(X_test_con_en,y_test_con_en,y_pred_con_en, input_length, output_length, 'SVR')


# Function for evaluating Mean Absolute Percentage Error (MAPE)
def mape_evaluation(X_test, y_test, y_preds, algorithm):
    # Calculate Mean Absolute Percentage Error (MAPE) for model
    mape_algo = mape(y_preds.reshape(1, -1), y_test.reshape(1, -1))
    
    
    # Generate the bar plot
    fig, ax = plt.subplots()
    x = [algorithm]
    y = [mape_algo]
    ax.bar(x, y, width=0.4)
    ax.set_xlabel('Regressor models')
    ax.set_ylabel('MAPE (%)')
    
    # Add text labels above each bar to display the corresponding MAPE value
    for index, value in enumerate(y):
        plt.text(x=index, y=value + 0.05, s=str(value), ha='center')
    
    # Adjust layout for better readability
    plt.tight_layout()


mape_evaluation(X_test_con_en, y_test_con_en, y_pred_con_en, 'svr')















